# -*- coding: utf-8 -*-
# """Llama2InferenceCleanedipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1dUX233ZXrUiFKt5uDcCnaC0luKUG3WKs
# """

## <IMPORTS>

import os
import torch
import gc
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel

from trl import SFTTrainer

import transformers
from torch import cuda


def Get_ModelTokenizer():
    # model_name = "NousResearch/Llama-2-7b-chat-hf"
    model_name = "Domwerr/llama2-7b-Emoji-epoch4-2e-4"

    # Activate 4-bit precision base model loading
    use_4bit = True
    # Compute dtype for 4-bit base models
    bnb_4bit_compute_dtype = "float16"
    # Quantization type (fp4 or nf4)
    bnb_4bit_quant_type = "nf4"
    # Activate nested quantization for 4-bit base models (double quantization)
    use_nested_quant = False
    # Load tokenizer and model with QLoRA configuration
    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=use_4bit,
        bnb_4bit_quant_type=bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=use_nested_quant,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    # tokenizer.pad_token = "[PAD]"
    # tokenizer.padding_side = "left"
    # Define PAD Token = BOS Token

    # Load base model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto"
        ,low_cpu_mem_usage=True
    )

    tokenizer.pad_token = tokenizer.bos_token
    model.config.pad_token_id = model.config.bos_token_id

    return model, tokenizer

def Get_Current_Device():
    from torch import cuda
    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
    print(device)
    return device

def GetInference(SentenceInsert, model, tokenizer, device):
    #Inference
    # sentences = ['''Please create a sentence from these 3 aspects: women_holding_hands_medium_skin_tone_medium-dark_skin_tone, woman_in_lotus_position_dark_skin_tone, woman_getting_haircut_medium_skin_tone. ''']
    sentences = [SentenceInsert]


    inputs = tokenizer(sentences, return_tensors="pt", padding=True).to(model.device)
    print(inputs['input_ids'].shape)

    output_sequences = model.generate(**inputs, max_new_tokens=100, min_new_tokens=5,
                                    # output_scores=True,
                                    do_sample=True,
                                    temperature=1.0,)

    ResultOfInference = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)
    # print(ResultOfInference[0])

    #PostProcessing
    LenOfPromptSent = len(sentences[0])
    SplitResult = ResultOfInference[0][LenOfPromptSent+1:].split(".")
    ResultSplit = SplitResult[0].split("Result: ")
    # ActualResult = SplitResult[0][7:] + "."
    # Garbage Collection
    del inputs
    del output_sequences
    del ResultOfInference
    gc.collect()

    torch.cuda.empty_cache()
    return ResultSplit[1]

if __name__ == '__main__' :
    CurModel, CurTokenizer = Get_ModelTokenizer()
    CurDevice = Get_Current_Device()


    CurSentence = "Please create a sentence from these 3 aspects: women_holding_hands_medium_skin_tone_medium-dark_skin_tone, woman_in_lotus_position_dark_skin_tone, woman_getting_haircut_medium_skin_tone."
    CurSentence1 = "Please create a sentence from these 3 aspects: kissing_face_with_closed_eyes, worried_face, alien."
    Result = GetInference(CurSentence1, CurModel, CurTokenizer, CurDevice)
    print("=================BATAS SUCI==================")
    print(Result)