# -*- coding: utf-8 -*-
# """Llama2InferenceCleanedipynb

# Automatically generated by Colaboratory.

## <IMPORTS>
import os
import torch
import gc
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel

from trl import SFTTrainer

import transformers
from torch import cuda
import re


def Get_ModelTokenizer():
    # model_name = "NousResearch/Llama-2-7b-chat-hf"
    model_name = "Domwerr/llama2-7b-Emoji-epoch4-2e-4" #EmojitoSentence
    # model_name = "Domwerr/llama2-7b-EmojiToResponse-epoch4-2e-4" #EmojitoResponse
    # model_name = "Domwerr/llama2-7b-Emoji-epoch4-2e-4-test"

    #QLoRA configuration
    compute_dtype = getattr(torch, "float16")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

    # Loading Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    # Load base model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto"
        ,low_cpu_mem_usage=True
    )

    tokenizer.pad_token = tokenizer.bos_token
    model.config.pad_token_id = model.config.bos_token_id

    return model, tokenizer

def Get_Current_Device():
    from torch import cuda
    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
    print(device)
    return device

def GetInference(SentenceInsert, model, tokenizer, device):
    #Inference
    # sentences = ['''Please create a sentence from these 3 aspects: women_holding_hands_medium_skin_tone_medium-dark_skin_tone, woman_in_lotus_position_dark_skin_tone, woman_getting_haircut_medium_skin_tone. ''']
    sentences = [SentenceInsert]


    inputtokenized = tokenizer(sentences, return_tensors="pt", padding=True).to(model.device)
    print(inputtokenized['input_ids'].shape)

    output_sequences = model.generate(**inputtokenized, max_new_tokens=80, min_new_tokens=5,
                                    # output_scores=True,
                                    do_sample=True,
                                    temperature=1.0,)

    ResultOfInference = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)
    # print(ResultOfInference[0])

    #PostProcessing
    LenOfPromptSent = len(sentences[0])
    SplitResult = ResultOfInference[0][LenOfPromptSent+1:].split(".")
    ResultSplit = SplitResult[0].split("Result: ")
    # ActualResult = SplitResult[0][7:] + "."
    # Garbage Collection
    del inputtokenized
    del output_sequences
    del ResultOfInference
    gc.collect()

    torch.cuda.empty_cache()
    return ResultSplit[1]


#==========================Emoji To Response
def filter_string(input_string):
    # Regular expression to match only the specified characters
    filtered_string = re.sub(r'[^a-zA-Z ?.!]', '', input_string)
    filtered_string = re.sub(r' +', ' ', filtered_string)
    return filtered_string

def Get_ModelTokenizer_ETR():
    # model_name = "NousResearch/Llama-2-7b-chat-hf"
    # model_name = "Domwerr/llama2-7b-Emoji-epoch4-2e-4" #EmojitoSentence
    model_name = "Domwerr/llama2-7b-EmojiToResponse-epoch4-2e-4" #EmojitoResponse
    # model_name = "Domwerr/llama2-7b-Emoji-epoch4-2e-4-test"

    #QLoRA configuration
    compute_dtype = getattr(torch, "float16")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

    # Loading Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    # Loading Model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto"
        ,low_cpu_mem_usage=True
    )

    tokenizer.pad_token = tokenizer.bos_token
    model.config.pad_token_id = model.config.bos_token_id

    return model, tokenizer

def Get_Current_Device_ETR():
    from torch import cuda
    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
    return device

def GetInference_ETR(SentenceInsert, model, tokenizer, device):
    #Inference
    # sentences = ['''Please create a sentence from these 3 aspects: women_holding_hands_medium_skin_tone_medium-dark_skin_tone, woman_in_lotus_position_dark_skin_tone, woman_getting_haircut_medium_skin_tone. ''']
    sentences = [SentenceInsert]


    inputtokenized = tokenizer(sentences, return_tensors="pt", padding=True).to(model.device)
    print(inputtokenized['input_ids'].shape)

    output_sequences = model.generate(**inputtokenized, max_new_tokens=80, min_new_tokens=5,
                                    # output_scores=True,
                                    do_sample=True,
                                    temperature=1.0,)

    ResultOfInference = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)
    # print(ResultOfInference[0])

    #PostProcessing
    DotMark = 0
    QuestionMark = 0
    ExclamationMark = 0

    ActualResult = "Sorry, we could not generate a response for this input."
    LenOfPromptSent = len(sentences[0])
    NonPrompt = ResultOfInference[0][LenOfPromptSent:]
    if(NonPrompt.find("Result:")==-1):
      ActualResult = "Sorry, we could not generate a response for this input."
    else:
      SplitResult2 = NonPrompt.split("?")
      if(len(SplitResult2)==1):
        DotMark = 1
      else:
        QuestionMark = 1
      SplitResult3 = SplitResult2[0].split("!")
      if(len(SplitResult3)==1):
        DotMark = 1
      else:
        ExclamationMark = 1

      if(ExclamationMark==1):
        ActualResult = SplitResult3[0][9:] + "!"
      elif(QuestionMark==1):
        ActualResult = SplitResult3[0][9:] + "?"
      else:
        ActualResult = SplitResult3[0][9:] + "."

    ActualResult = filter_string(ActualResult)


    # Garbage Collection
    del inputtokenized
    del output_sequences
    del ResultOfInference
    gc.collect()

    torch.cuda.empty_cache()
    return ActualResult




if __name__ == '__main__' :
    # CurModel, CurTokenizer = Get_ModelTokenizer()
    # CurDevice = Get_Current_Device()


    # CurSentence = "Please create a sentence from these 3 aspects: women_holding_hands_medium_skin_tone_medium-dark_skin_tone, woman_in_lotus_position_dark_skin_tone, woman_getting_haircut_medium_skin_tone."
    # CurSentence1 = "Please create a sentence from these 3 aspects: kissing_face_with_closed_eyes, worried_face, alien."
    # Result = GetInference(CurSentence1, CurModel, CurTokenizer, CurDevice)


    #check ETR
    CurModelETR, CurTokenizerETR = Get_ModelTokenizer_ETR()
    CurDevice = Get_Current_Device_ETR()
    CurSentenceETR = "Please respond the following text as if in a chat using only one sentence: man_playing_water_polo_medium_skin_tone, women_holding_hands_medium_skin_tone_dark_skin_tone, man_police_officer."
    Result = GetInference_ETR(CurSentenceETR, CurModelETR, CurTokenizerETR, CurDevice)
    print("=================BATAS SUCI==================")
    print(Result)